{"cells":[{"cell_type":"markdown","metadata":{"id":"v0zU0_w4VWBn"},"source":["# Transfering and Fine Tuning Transformer for MultiLabel Text Classification"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:12.239200Z","iopub.status.busy":"2023-12-26T16:35:12.238422Z","iopub.status.idle":"2023-12-26T16:35:26.165326Z","shell.execute_reply":"2023-12-26T16:35:26.164209Z","shell.execute_reply.started":"2023-12-26T16:35:12.239153Z"},"id":"WD_vnyLXZQzD","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Installing the transformers library and additional libraries if looking process\n","\n","%pip install -q transformers\n","\n","# Code for TPU packages install\n","# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:26.168301Z","iopub.status.busy":"2023-12-26T16:35:26.167888Z","iopub.status.idle":"2023-12-26T16:35:35.974164Z","shell.execute_reply":"2023-12-26T16:35:35.973360Z","shell.execute_reply.started":"2023-12-26T16:35:26.168262Z"},"id":"pzM1_ykHaFur","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig\n","import ast\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"execution":{"iopub.execute_input":"2023-12-26T16:35:35.975687Z","iopub.status.busy":"2023-12-26T16:35:35.975264Z","iopub.status.idle":"2023-12-26T16:35:36.047754Z","shell.execute_reply":"2023-12-26T16:35:36.046669Z","shell.execute_reply.started":"2023-12-26T16:35:35.975659Z"},"id":"NLxxwd1scQNv","outputId":"fd435f34-8083-41d5-dea6-5e57d558e513","trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# # Setting up the device for GPU usage\n","\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"id":"VPYp6oKWVWBs"},"source":["<a id='section02'></a>\n","### Importing and Pre-Processing the domain data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.050484Z","iopub.status.busy":"2023-12-26T16:35:36.049795Z","iopub.status.idle":"2023-12-26T16:35:36.057716Z","shell.execute_reply":"2023-12-26T16:35:36.056762Z","shell.execute_reply.started":"2023-12-26T16:35:36.050452Z"},"id":"__rGBi4zkvBx","trusted":true},"outputs":[],"source":["movie_train_path = 'cleaned_data\\movies_train_augmented.csv'\n","movie_test_path = 'cleaned_data\\movies_test.csv'\n","genre_path = 'cleaned_data\\genres.txt'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.059052Z","iopub.status.busy":"2023-12-26T16:35:36.058790Z","iopub.status.idle":"2023-12-26T16:35:36.170039Z","shell.execute_reply":"2023-12-26T16:35:36.169174Z","shell.execute_reply.started":"2023-12-26T16:35:36.059030Z"},"id":"f0bHuktFkvBx","trusted":true},"outputs":[],"source":["\n","movies_train = pd.read_csv(movie_train_path, engine= 'python',\n","                         encoding='latin-1', index_col=False).set_index('movieid')\n","movies_test = pd.read_csv(movie_test_path, engine='python',\n","                         encoding='latin-1', index_col=False).set_index('movieid')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.171252Z","iopub.status.busy":"2023-12-26T16:35:36.170999Z","iopub.status.idle":"2023-12-26T16:35:36.275964Z","shell.execute_reply":"2023-12-26T16:35:36.275316Z","shell.execute_reply.started":"2023-12-26T16:35:36.171230Z"},"id":"01_weS-xkvBx","trusted":true},"outputs":[],"source":["movies_train['genre'] = movies_train['genre'].apply(ast.literal_eval)\n","movies_test['genre'] = movies_test['genre'].apply(ast.literal_eval)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.277295Z","iopub.status.busy":"2023-12-26T16:35:36.277016Z","iopub.status.idle":"2023-12-26T16:35:36.295214Z","shell.execute_reply":"2023-12-26T16:35:36.294359Z","shell.execute_reply.started":"2023-12-26T16:35:36.277270Z"},"id":"bFJZC1tBkvBx","outputId":"1c6c4c98-de56-46a6-d3c9-4b0781b1e8c3","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>genre</th>\n","    </tr>\n","    <tr>\n","      <th>movieid</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1650</th>\n","      <td>Washington Square</td>\n","      <td>[Drama]</td>\n","    </tr>\n","    <tr>\n","      <th>185</th>\n","      <td>The Net</td>\n","      <td>[Sci-Fi, Thriller]</td>\n","    </tr>\n","    <tr>\n","      <th>1377</th>\n","      <td>Batman Returns</td>\n","      <td>[Action, Adventure, Comedy, Crime]</td>\n","    </tr>\n","    <tr>\n","      <th>3204</th>\n","      <td>The Boys from Brazil</td>\n","      <td>[Thriller]</td>\n","    </tr>\n","    <tr>\n","      <th>1901</th>\n","      <td>Dear Jesse</td>\n","      <td>[Documentary]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1750</th>\n","      <td>Adept</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","    </tr>\n","    <tr>\n","      <th>1750</th>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","    </tr>\n","    <tr>\n","      <th>1750</th>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","    </tr>\n","    <tr>\n","      <th>1750</th>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","    </tr>\n","    <tr>\n","      <th>1750</th>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8781 rows × 2 columns</p>\n","</div>"],"text/plain":["                        title                                     genre\n","movieid                                                                \n","1650        Washington Square                                   [Drama]\n","185                   The Net                        [Sci-Fi, Thriller]\n","1377           Batman Returns        [Action, Adventure, Comedy, Crime]\n","3204     The Boys from Brazil                                [Thriller]\n","1901               Dear Jesse                             [Documentary]\n","...                       ...                                       ...\n","1750                    Adept  [Adventure, Children's, Fantasy, Sci-Fi]\n","1750                     Good  [Adventure, Children's, Fantasy, Sci-Fi]\n","1750                     Good  [Adventure, Children's, Fantasy, Sci-Fi]\n","1750                     Good  [Adventure, Children's, Fantasy, Sci-Fi]\n","1750                     Good  [Adventure, Children's, Fantasy, Sci-Fi]\n","\n","[8781 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["movies_train"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.296547Z","iopub.status.busy":"2023-12-26T16:35:36.296256Z","iopub.status.idle":"2023-12-26T16:35:36.310907Z","shell.execute_reply":"2023-12-26T16:35:36.310015Z","shell.execute_reply.started":"2023-12-26T16:35:36.296520Z"},"id":"ceJ7ySDckvBy","outputId":"94644997-09a9-473b-e189-7866c1b37703","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["18\n","{'Crime': 0, 'Thriller': 1, 'Fantasy': 2, 'Horror': 3, 'Sci-Fi': 4, 'Comedy': 5, 'Documentary': 6, 'Adventure': 7, 'Film-Noir': 8, 'Animation': 9, 'Romance': 10, 'Drama': 11, 'Western': 12, 'Musical': 13, 'Action': 14, 'Mystery': 15, 'War': 16, \"Children's\": 17}\n"]}],"source":["with open(genre_path, 'r') as f:\n","    genre_all = f.readlines()\n","    genre_all = [x.replace('\\n','') for x in genre_all]\n","genre2idx = {genre:idx for idx, genre in enumerate(genre_all)}\n","idx2genre = {idx: genre for idx, genre in enumerate(genre_all)}\n","num_classes = len(genre2idx)\n","print(num_classes)\n","print(genre2idx)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.312596Z","iopub.status.busy":"2023-12-26T16:35:36.312015Z","iopub.status.idle":"2023-12-26T16:35:36.317751Z","shell.execute_reply":"2023-12-26T16:35:36.316699Z","shell.execute_reply.started":"2023-12-26T16:35:36.312564Z"},"id":"Sk0pirdgkvBy","trusted":true},"outputs":[],"source":["def label_to_vecto(genre: list[str]):\n","    genre_vector = np.zeros(len(genre2idx))\n","\n","    for g in genre:\n","        genre_vector[genre2idx[g]] = 1\n","\n","    return genre_vector.tolist()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.321669Z","iopub.status.busy":"2023-12-26T16:35:36.321391Z","iopub.status.idle":"2023-12-26T16:35:36.359605Z","shell.execute_reply":"2023-12-26T16:35:36.358758Z","shell.execute_reply.started":"2023-12-26T16:35:36.321647Z"},"id":"N7nrpCBTkvBy","trusted":true},"outputs":[],"source":["movies_train['vecto'] = movies_train['genre'].apply(label_to_vecto)\n","movies_test['vecto'] = movies_test['genre'].apply(label_to_vecto)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:36.361015Z","iopub.status.busy":"2023-12-26T16:35:36.360692Z","iopub.status.idle":"2023-12-26T16:35:36.366210Z","shell.execute_reply":"2023-12-26T16:35:36.365363Z","shell.execute_reply.started":"2023-12-26T16:35:36.360982Z"},"id":"95U8wZqNkvBy","trusted":true},"outputs":[],"source":["movies_train.reset_index(inplace= True)\n","movies_test.reset_index(inplace= True)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.367992Z","iopub.status.busy":"2023-12-26T16:35:36.367674Z","iopub.status.idle":"2023-12-26T16:35:36.392248Z","shell.execute_reply":"2023-12-26T16:35:36.391316Z","shell.execute_reply.started":"2023-12-26T16:35:36.367962Z"},"id":"lAYZZHZkkvBy","outputId":"e4f9f290-2b3d-4d91-cdf6-c9b58ad2cb15","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>movieid</th>\n","      <th>title</th>\n","      <th>genre</th>\n","      <th>vecto</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1650</td>\n","      <td>Washington Square</td>\n","      <td>[Drama]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>185</td>\n","      <td>The Net</td>\n","      <td>[Sci-Fi, Thriller]</td>\n","      <td>[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1377</td>\n","      <td>Batman Returns</td>\n","      <td>[Action, Adventure, Comedy, Crime]</td>\n","      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3204</td>\n","      <td>The Boys from Brazil</td>\n","      <td>[Thriller]</td>\n","      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1901</td>\n","      <td>Dear Jesse</td>\n","      <td>[Documentary]</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8776</th>\n","      <td>1750</td>\n","      <td>Adept</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8777</th>\n","      <td>1750</td>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8778</th>\n","      <td>1750</td>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8779</th>\n","      <td>1750</td>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8780</th>\n","      <td>1750</td>\n","      <td>Good</td>\n","      <td>[Adventure, Children's, Fantasy, Sci-Fi]</td>\n","      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8781 rows × 4 columns</p>\n","</div>"],"text/plain":["      movieid                 title                                     genre  \\\n","0        1650     Washington Square                                   [Drama]   \n","1         185               The Net                        [Sci-Fi, Thriller]   \n","2        1377        Batman Returns        [Action, Adventure, Comedy, Crime]   \n","3        3204  The Boys from Brazil                                [Thriller]   \n","4        1901            Dear Jesse                             [Documentary]   \n","...       ...                   ...                                       ...   \n","8776     1750                 Adept  [Adventure, Children's, Fantasy, Sci-Fi]   \n","8777     1750                  Good  [Adventure, Children's, Fantasy, Sci-Fi]   \n","8778     1750                  Good  [Adventure, Children's, Fantasy, Sci-Fi]   \n","8779     1750                  Good  [Adventure, Children's, Fantasy, Sci-Fi]   \n","8780     1750                  Good  [Adventure, Children's, Fantasy, Sci-Fi]   \n","\n","                                                  vecto  \n","0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","1     [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n","2     [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n","3     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n","...                                                 ...  \n","8776  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n","8777  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n","8778  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n","8779  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n","8780  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n","\n","[8781 rows x 4 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["movies_train"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.393903Z","iopub.status.busy":"2023-12-26T16:35:36.393285Z","iopub.status.idle":"2023-12-26T16:35:36.401356Z","shell.execute_reply":"2023-12-26T16:35:36.400515Z","shell.execute_reply.started":"2023-12-26T16:35:36.393869Z"},"id":"cQBVrqWjkvBy","outputId":"12a14730-52b4-4545-f60f-09cd114543cf","trusted":true},"outputs":[{"data":{"text/plain":["str"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["type(movies_train['title'].iloc[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.402892Z","iopub.status.busy":"2023-12-26T16:35:36.402610Z","iopub.status.idle":"2023-12-26T16:35:36.414165Z","shell.execute_reply":"2023-12-26T16:35:36.413398Z","shell.execute_reply.started":"2023-12-26T16:35:36.402867Z"},"id":"sk3WXTsCkvBy","outputId":"2218ebcc-925a-4764-c744-38aa45814b99","trusted":true},"outputs":[{"data":{"text/plain":["list"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["type(movies_train['vecto'].iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"f-YRFfhJVWBs"},"source":["<a id='section03'></a>\n","### Preparing the Dataset and Dataloader"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["95787fc7201249f2b8f199c510baff49","4b2175016c8b404b83ef3aaee33f4a02","74dcefc076f74ed58bbc9d68dd66672a","6cce1a5b72ee43af8cd5e99dd8ab566f","72d323d2f243441db62a40cb87242cb7","cc60d1440a684829baead79cb73f4a00","42476d7b48dc4603a219433ef72d73f9","6303582351254df298ed332f9ad12dde","5d78c7d3bbdc4f2ab6398aef2b4a4995","d0c5a91a9bb24c5abb5d55b42f219ef5","25747b914e48410089948dbae7a72481","db9f57a9d3b849658a6fa3c2bce4b5eb","8551f174a5cc4aa09577b6bfc215a710","d0c9fefffd304b6fbe26f23becd59d71","8353642ab3044b46ac69318c49444674","fb935a7d0e0c40b59f1d0d42eafe4aaa","59dfed5429824034a6f7d92fd24be2d9","ef2a59914e154a079bf815228cbc5625","2684e8b15f5441ae82640a8dd6189f3a","f01a9d7b959a46f99befa2db4fab40e2","a2c5917566f44ad6915fc5aad39a385a","68c49f40eab74f5a855db11c980ff400","2f06ba7db7e240a1aae07c6284521f93","f8a144e98341455db4de7bdc6e71fba7","72426db91e5648b08099fbb68c44d3cc","877485e2ccb64f1c8e4b5a1745028982","d0390137623e472db4623fd8dc36628e","4630f7981b384ed6ad4827c077730315","a5ff6fa6b00d4c5ba318403dfd1b53bb","8c296a0603a441a6a858f573cd925e51","f03e9f0580cf40d1859f2c036394877f","49ab0a9a8ff645dd9098161091ec2d37","d32fc9dd58aa42cca3948ba02a24286b","4f5c15b8959449daa3b93452231bd061","0643118f70a349f7b710fd3ba2f7b309","edc4d253b55748c7bd01492f6aadb41d","f683968b196f40099929b2961bd7d2cb","dfc4bf9cf4084176a8eec9a742ae0333","dc6683e723d14258aa55e89fd1ed484b","d778c5decf30449bb889054a5c27d2b1","f009a540f6e943fdb88b66abfb94977e","52238ba05c314fd087acdfcb66d86018","8ed17004d5164e87ab18e0de32c8a077","9fe42b3247af469cb43d89bbf68494a7"]},"execution":{"iopub.execute_input":"2023-12-26T16:35:36.415948Z","iopub.status.busy":"2023-12-26T16:35:36.415669Z","iopub.status.idle":"2023-12-26T16:35:37.451959Z","shell.execute_reply":"2023-12-26T16:35:37.450995Z","shell.execute_reply.started":"2023-12-26T16:35:36.415923Z"},"id":"ikfbFlNHgi8T","outputId":"1db2ae6a-11d6-4dfe-b161-23f9feac81bd","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ad9c6cd11be45138d24b8b8be6dad34","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e84663436d440688694d5ece3a42bbf","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8283bba4520b4728b9ded0e60ddc9ffe","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b98f8132b1a64098a27461a17bda97dc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Sections of config\n","\n","# Defining some key variables that will be used later on in the training\n","MAX_LEN = 7\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:37.453479Z","iopub.status.busy":"2023-12-26T16:35:37.453180Z","iopub.status.idle":"2023-12-26T16:35:37.462952Z","shell.execute_reply":"2023-12-26T16:35:37.462032Z","shell.execute_reply.started":"2023-12-26T16:35:37.453453Z"},"id":"oFOylAXqiNYK","trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.comment_text = dataframe.title\n","        self.targets = self.data.vecto\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        comment_text = str(self.comment_text[index])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            # padding= \"max_length\",\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:37.464471Z","iopub.status.busy":"2023-12-26T16:35:37.464113Z","iopub.status.idle":"2023-12-26T16:35:37.476673Z","shell.execute_reply":"2023-12-26T16:35:37.475782Z","shell.execute_reply.started":"2023-12-26T16:35:37.464437Z"},"id":"PkDGqarcPowL","outputId":"0cbece92-e742-49bb-e083-7ccc8090e4f8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAIN Dataset: (8781, 4)\n","TEST Dataset: (777, 4)\n"]}],"source":["# Creating the dataset and dataloader for the neural network\n","\n","print(\"TRAIN Dataset: {}\".format(movies_train.shape))\n","print(\"TEST Dataset: {}\".format(movies_test.shape))\n","\n","training_set = CustomDataset(movies_train, tokenizer, MAX_LEN)\n","testing_set = CustomDataset(movies_test, tokenizer, MAX_LEN)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:37.478213Z","iopub.status.busy":"2023-12-26T16:35:37.477954Z","iopub.status.idle":"2023-12-26T16:35:37.657005Z","shell.execute_reply":"2023-12-26T16:35:37.656094Z","shell.execute_reply.started":"2023-12-26T16:35:37.478190Z"},"id":"KceQR_ZDkvBz","outputId":"c09cc1c1-cc28-4114-a050-11a1d413f3b5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["{'ids': tensor([ 101, 2899, 2675,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 1996, 5658,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 8942, 5651,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])}\n","{'ids': tensor([ 101, 1996, 3337, 2013, 4380,  102,    0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 6203, 7627,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([  101,  1996, 15723,   102,     0,     0,     0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 2358, 8490,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])}\n","{'ids': tensor([ 101, 2610, 2914, 1019, 1024, 8775,  102]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 1996, 2203, 1997, 1996, 6771,  102]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 2178, 2154, 1999, 9097,  102,    0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([  101,  7090, 10487,   102,     0,     0,     0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 8618, 4231,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 4111, 2160,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 2293, 2003, 1996, 6548,  102,    0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([  101, 16608,  1996, 25075,   102,     0,     0]), 'mask': tensor([1, 1, 1, 1, 1, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])}\n","{'ids': tensor([ 101, 9649, 1005, 1055, 4672,  102,    0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])}\n","{'ids': tensor([ 101, 2137, 3959,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 1996, 3536,  102,    0,    0,    0]), 'mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([  101, 17266,  3597,  1998,  3158,  4371,   102]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}\n","{'ids': tensor([ 101, 8942, 1024, 7308, 1997, 1996,  102]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])}\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["n = 0\n","for i in training_set:\n","    n += 1\n","    print(i)\n","    if n == 20: break\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:37.658369Z","iopub.status.busy":"2023-12-26T16:35:37.658122Z","iopub.status.idle":"2023-12-26T16:35:37.663263Z","shell.execute_reply":"2023-12-26T16:35:37.662426Z","shell.execute_reply.started":"2023-12-26T16:35:37.658347Z"},"id":"vLpilV73QrXJ","outputId":"9d92c336-a817-4240-d357-36922150e2cf","trusted":true},"outputs":[],"source":["training_loader = DataLoader(training_set, batch_size= TRAIN_BATCH_SIZE, shuffle= True, num_workers= 4)\n","testing_loader = DataLoader(testing_set, batch_size= VALID_BATCH_SIZE, shuffle= False, num_workers= 4)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:37.664533Z","iopub.status.busy":"2023-12-26T16:35:37.664266Z","iopub.status.idle":"2023-12-26T16:35:37.860781Z","shell.execute_reply":"2023-12-26T16:35:37.859623Z","shell.execute_reply.started":"2023-12-26T16:35:37.664511Z"},"id":"ZoUmpNYykvBz","outputId":"b11ef15d-cf7a-4180-d628-e68df6594d74","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  101,  1996,  9012,   102,     0,     0,     0],\n","        [  101,  1996,  3521,  8571,   102,     0,     0],\n","        [  101, 12801,   102,     0,     0,     0,     0],\n","        [  101,  3340,  1998,  6963,   102,     0,     0],\n","        [  101,  2012,  2034,  4356,   102,     0,     0],\n","        [  101,  1996,  2712,  1997,  2051,   102,     0],\n","        [  101, 10905, 29099,  1997,  4356,   102,     0],\n","        [  101, 10108,   102,     0,     0,     0,     0],\n","        [  101,  6643,  8197, 22179,   102,     0,     0],\n","        [  101,  9932, 19784, 14071,  6342,  2072,   102],\n","        [  101, 27669,  5544,   102,     0,     0,     0],\n","        [  101,  3336,  7959,  6299,   102,     0,     0],\n","        [  101,  2812,  5943,  5943,  2154,  2217,   102],\n","        [  101,  7619,  2373,   102,     0,     0,     0],\n","        [  101,  2005,  1996,  2293,  1997, 27231,   102],\n","        [  101, 13294,   102,     0,     0,     0,     0]])\n"]}],"source":["for batch in training_loader:\n","    print(batch['ids'])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"fF_7DIbDVWBt"},"source":["<a id='section04'></a>\n","### Creating the Neural Network "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":830,"referenced_widgets":["c58803afc6884d70b28918665b57d5a6","5ff2b089b333430dbc03216378418b39","963eff7f36ff451caf8b465e4ef960fa","b6c47d22180f405cab3983ae9075ffc0","2f3bef22a0434064a44efb3fdb6854b0","5afd07fe16ff49a3b14741e65b5d9bb3","dbd4ac156cbd43398c02bd3b64203f97","029b65c15821481980dcdd409dc07509","9a7834964e5d4019b73a719756c90537","cbe66af82a384db19971b1e035ee0f71","783f8d7814f144e78fecf6a24ce0a216"]},"execution":{"iopub.execute_input":"2023-12-26T16:35:37.862685Z","iopub.status.busy":"2023-12-26T16:35:37.862338Z","iopub.status.idle":"2023-12-26T16:35:45.759941Z","shell.execute_reply":"2023-12-26T16:35:45.759005Z","shell.execute_reply.started":"2023-12-26T16:35:37.862654Z"},"id":"DegHNyIEQxB2","outputId":"7b9dfdd5-efcf-44e5-cff8-f27f4be5481d","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76d1836d511a400aa69d35af515c8440","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["BERTClass(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (l2): Dropout(p=0.3, inplace=False)\n","  (l3): Linear(in_features=768, out_features=18, bias=True)\n",")"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n","\n","class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n","        self.l2 = torch.nn.Dropout(0.3)\n","        self.l3 = torch.nn.Linear(768, num_classes)\n","\n","    def forward(self, ids, mask, token_type_ids):\n","        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n","        output_2 = self.l2(output_1)\n","        output = self.l3(output_2)\n","        return output\n","\n","model = BERTClass()\n","model.to(device)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:45.761393Z","iopub.status.busy":"2023-12-26T16:35:45.761113Z","iopub.status.idle":"2023-12-26T16:35:46.275006Z","shell.execute_reply":"2023-12-26T16:35:46.274004Z","shell.execute_reply.started":"2023-12-26T16:35:45.761369Z"},"id":"xFeiWnv4yDMw","outputId":"078a65f4-1792-4ac4-d371-2cb4d8a51d96","trusted":true},"outputs":[{"data":{"text/plain":["BERTClass(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (l2): Dropout(p=0.3, inplace=False)\n","  (l3): Linear(in_features=768, out_features=18, bias=True)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["class BERTClass2(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass2, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = torch.nn.Dropout(0.3)\n","\n","        self.fc1 = torch.nn.Linear(768, 320)\n","        self.relu1 = torch.nn.ReLU()\n","        self.dropout1 = torch.nn.Dropout(0.3)\n","\n","\n","        self.fc2 = torch.nn.Linear(320, 160)\n","        self.relu2 = torch.nn.ReLU()\n","        self.dropout2 = torch.nn.Dropout(0.3)\n","\n","        self.fc3 = torch.nn.Linear(160, num_classes)\n","\n","\n","    def forward(self, ids, mask, token_type_ids):\n","        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n","        x = self.dropout(output_1)\n","\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","\n","        return x\n","\n","model = BERTClass()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"ANheywbhVWBu"},"source":["<a id='section05'></a>\n","### Transfer and Fine Tune the Model\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:46.277067Z","iopub.status.busy":"2023-12-26T16:35:46.276421Z","iopub.status.idle":"2023-12-26T16:35:46.281241Z","shell.execute_reply":"2023-12-26T16:35:46.280400Z","shell.execute_reply.started":"2023-12-26T16:35:46.277028Z"},"id":"eWVXS50KkvBz","outputId":"484e7d8f-4ae3-4e0b-8e3b-19c81b46128c","trusted":true},"outputs":[],"source":["# for i, param in enumerate(model.parameters()):\n","#     print(f'{i} \\t {param.shape}')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:46.282905Z","iopub.status.busy":"2023-12-26T16:35:46.282528Z","iopub.status.idle":"2023-12-26T16:35:46.294487Z","shell.execute_reply":"2023-12-26T16:35:46.293797Z","shell.execute_reply.started":"2023-12-26T16:35:46.282868Z"},"id":"jx4XQA0BkvB0","trusted":true},"outputs":[],"source":["def set_parameter_requires_grad(model, num_layers_to_freeze = 199):\n","    \"\"\"\n","    Freeze the first num_layers_to_freeze groups of layers in the model.\n","    \"\"\"\n","    for i, param in enumerate(model.parameters()):\n","        if i < num_layers_to_freeze:\n","            param.requires_grad = False\n","        else:\n","            break"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:46.295920Z","iopub.status.busy":"2023-12-26T16:35:46.295608Z","iopub.status.idle":"2023-12-26T16:35:46.304220Z","shell.execute_reply":"2023-12-26T16:35:46.303570Z","shell.execute_reply.started":"2023-12-26T16:35:46.295897Z"},"id":"65fsefvRsmem","trusted":true},"outputs":[],"source":["def get_base_model():\n","  return BERTClass2()\n","\n","def get_model_with_name(name, weight_file= None):\n","    model = None\n","    if name == 'model1':\n","        model = BERTClass()\n","    else:\n","        model = BERTClass2()\n","    if weight_file:\n","        print(\"load weight from\", weight_file)\n","        model.load_state_dict(torch.load(weight_file))\n","    return model"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:46.305651Z","iopub.status.busy":"2023-12-26T16:35:46.305280Z","iopub.status.idle":"2023-12-26T16:35:46.316107Z","shell.execute_reply":"2023-12-26T16:35:46.315367Z","shell.execute_reply.started":"2023-12-26T16:35:46.305624Z"},"id":"IHQy0jHhkvB0","trusted":true},"outputs":[],"source":["def get_model(mode, weight_file= None):\n","    # Assuming your BERT model is named 'model'\n","    model = get_base_model()\n","    if mode == 'transfer':\n","        # Freeze the first 11 groups of layers for transfer learning\n","        set_parameter_requires_grad(model, num_layers_to_freeze=199)\n","\n","    else:\n","        # Unfreeze all layers for fine-tuning\n","        for param in model.parameters():\n","            param.requires_grad = True\n","\n","\n","    if weight_file:\n","        print(\"load weight from\", weight_file)\n","        model.load_state_dict(torch.load(weight_file))\n","\n","    return model"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T16:35:46.317285Z","iopub.status.busy":"2023-12-26T16:35:46.317054Z","iopub.status.idle":"2023-12-26T16:35:46.746017Z","shell.execute_reply":"2023-12-26T16:35:46.745026Z","shell.execute_reply.started":"2023-12-26T16:35:46.317264Z"},"id":"TwvjzCm3sBXV","outputId":"f6cb089b-2aa5-49b1-eee9-772ffc0e1e8a","trusted":true},"outputs":[{"data":{"text/plain":["BERTClass2(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=320, bias=True)\n","  (relu1): ReLU()\n","  (dropout1): Dropout(p=0.3, inplace=False)\n","  (fc2): Linear(in_features=320, out_features=160, bias=True)\n","  (relu2): ReLU()\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (fc3): Linear(in_features=160, out_features=18, bias=True)\n",")"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["model = get_model(mode='transfer')\n","model.to(device)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:46.752750Z","iopub.status.busy":"2023-12-26T16:35:46.752443Z","iopub.status.idle":"2023-12-26T16:35:46.756440Z","shell.execute_reply":"2023-12-26T16:35:46.755594Z","shell.execute_reply.started":"2023-12-26T16:35:46.752726Z"},"id":"1C7-EcjnsQ81","trusted":true},"outputs":[],"source":["# for i, param in enumerate(model.parameters()):\n","#     print(param)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T16:35:46.758343Z","iopub.status.busy":"2023-12-26T16:35:46.758011Z","iopub.status.idle":"2023-12-26T16:35:46.782695Z","shell.execute_reply":"2023-12-26T16:35:46.781789Z","shell.execute_reply.started":"2023-12-26T16:35:46.758315Z"},"id":"u8kTHihxkvB0","trusted":true},"outputs":[],"source":["def get_Loss_func():\n","    return torch.nn.BCEWithLogitsLoss()\n","\n","def model_step(batch, model, loss_func, device):\n","    ids = batch['ids'].to(device, dtype = torch.long)\n","    mask = batch['mask'].to(device, dtype = torch.long)\n","    token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n","    targets = batch['targets'].to(device, dtype = torch.float)\n","\n","    outputs = model(ids, mask, token_type_ids)\n","    loss = loss_func(outputs, targets)\n","    preds = outputs >= 0.5\n","\n","    return loss, preds, targets\n","\n","def optimizer_step(optimizer, loss):\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# def evaluation(model, loader, loss_func, epoch, device, stage):\n","#     model.eval()\n","#     total, total_correct, total_loss = 0, 0, 0\n","#     pbar = tqdm(loader)\n","#     for batch in pbar:\n","#         loss, preds, y = model_step(batch, model, loss_func, device)\n","#         # total += len(y) * num_classes\n","#         total += len(y)\n","#         total_loss += loss.item()\n","#         total_correct += torch.sum(preds == y).item()\n","#         pbar.set_description(f\"epoch = {epoch} val/acc = {total_correct/(total * num_classes):.3f}\")\n","#     return (total_loss/ total), (total_correct/(total * num_classes))    # loss, acc\n","\n","def evaluation(model, loader, loss_func, epoch, device, stage):\n","    model.eval()\n","    total, total_loss, total_preds, total_y = 0, 0, [], []\n","    pbar = tqdm(loader)\n","\n","    for batch in pbar:\n","        loss, preds, y = model_step(batch, model, loss_func, device)\n","\n","        total += len(y)\n","        total_loss += loss.item()\n","        total_preds.extend(preds.cpu().numpy())\n","        total_y.extend(y.cpu().numpy())\n","\n","        pbar.set_description(f\"epoch = {epoch} {stage}/loss = {total_loss/total:.3f}\")\n","\n","    f1 = metrics.f1_score(total_y, total_preds, average='micro')  # You can change 'weighted' to other options\n","    print(f'Validation F1 score: {f1}')\n","    return (total_loss / total), f1\n","\n","\n","# def train_epoch(model, train_loader, loss_func, optimizer, epoch, device):\n","#     model.train()\n","#     pbar = tqdm(train_loader)\n","#     total, total_loss, total_correct = 0, 0, 0\n","#     for batch in pbar:\n","#         loss, preds, y = model_step(batch, model, loss_func, device)\n","#         optimizer_step(optimizer, loss)\n","#         # total += len(y) * num_classes\n","#         total += len(y)\n","#         total_loss += loss.item()\n","#         total_correct += torch.sum(preds == y).item()\n","#         pbar.set_description(f\"epoch = {epoch} train/loss = {total_loss/total:.3f}\")\n","#     return (total_loss/ total), (total_correct/(total * num_classes))       #loss, acc\n","\n","def train_epoch(model, train_loader, loss_func, optimizer, epoch, device):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    total, total_loss, total_preds, total_y = 0, 0, [], []\n","\n","    for batch in pbar:\n","        loss, preds, y = model_step(batch, model, loss_func, device)\n","        optimizer_step(optimizer, loss)\n","\n","        total += len(y)\n","        total_loss += loss.item()\n","        total_preds.extend(preds.cpu().numpy())\n","        total_y.extend(y.cpu().numpy())\n","\n","        pbar.set_description(f\"epoch = {epoch} train/loss = {total_loss/total:.3f}\")\n","\n","    f1 = metrics.f1_score(total_y, total_preds, average='micro')\n","    return (total_loss / total), f1\n","\n","\n","def plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history):\n","    plt.figure(figsize=(8, 8))\n","    plt.subplot(2, 1, 1)\n","    plt.plot(train_acc_history, label='Training F1 Score')\n","    plt.plot(val_acc_history, label='Validation F1 Score')\n","    plt.legend(loc='lower right')\n","    plt.ylabel('Accuracy')\n","    plt.title('Training and Validation Accuracy')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.plot(train_loss_history, label='Training Loss')\n","    plt.plot(val_loss_history, label='Validation Loss')\n","    plt.legend(loc='upper right')\n","    plt.ylabel('BCElogits Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('epoch')\n","    plt.show()\n","\n","def train(model, train_loader, val_loader, lr=1e-3, max_epochs=1, device = \"cuda\", plot_res = True):\n","    train_loss_history = []\n","    train_acc_history = []\n","    val_loss_history = []\n","    val_acc_history = []\n","    loss_func = get_Loss_func()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    for epoch in range(max_epochs):\n","        ## train loop\n","        train_loss, train_acc = train_epoch(model, train_loader, loss_func, optimizer, epoch, device)\n","        train_loss_history.append(train_loss)\n","        train_acc_history.append(train_acc)\n","        ## val loop\n","        val_loss, val_acc = evaluation(model, val_loader, loss_func, epoch, device, \"val\")\n","        val_loss_history.append(val_loss)\n","        val_acc_history.append(val_acc)\n","\n","    if(plot_res):\n","        plot_training_history(train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n","    return (train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n","\n","def main(mode=\"fine-tune\", weight_file=None, output_file=None, device= 'cuda'):\n","\n","    if mode == \"fine-tune\" and weight_file:\n","        model = get_model(mode=mode, weight_file=weight_file)\n","    else:\n","        model = get_model(mode=mode)\n","    print(model)\n","    model.to(device)\n","    lr = 1e-3 if mode == \"transfer\" else 1e-5\n","    max_epochs = 15 if mode == \"transfer\" else 45\n","    train(model, training_loader, testing_loader, device=device, max_epochs=max_epochs, lr=lr)\n","\n","    if output_file:\n","        print(\"save model to\", output_file)\n","        torch.save(model.state_dict(), output_file)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-12-26T16:35:46.784161Z","iopub.status.busy":"2023-12-26T16:35:46.783836Z","iopub.status.idle":"2023-12-26T16:39:05.227506Z","shell.execute_reply":"2023-12-26T16:39:05.226341Z","shell.execute_reply.started":"2023-12-26T16:35:46.784130Z"},"id":"ifoL2pnpkvB0","outputId":"d048028e-003a-4c1a-ba5f-67ff417f8e9c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BERTClass2(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=320, bias=True)\n","  (relu1): ReLU()\n","  (dropout1): Dropout(p=0.3, inplace=False)\n","  (fc2): Linear(in_features=320, out_features=160, bias=True)\n","  (relu2): ReLU()\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (fc3): Linear(in_features=160, out_features=18, bias=True)\n",")\n"]},{"name":"stderr","output_type":"stream","text":["epoch = 0 train/loss = 0.020: 100%|██████████| 549/549 [00:16<00:00, 33.78it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 0 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 43.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 1 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 46.31it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 1 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 42.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 2 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.63it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 2 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 43.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 3 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.08it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 3 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 43.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 4 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.27it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 4 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 5 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 46.62it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 5 val/loss = 0.019: 100%|██████████| 49/49 [00:01<00:00, 41.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 6 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.85it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 6 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 7 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 7 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 8 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 46.99it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 8 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 9 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.34it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 9 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 45.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 10 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 46.85it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 10 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 45.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 11 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 48.19it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 11 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 12 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.71it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 12 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 42.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 13 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.14it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 13 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 44.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 14 train/loss = 0.019: 100%|██████████| 549/549 [00:11<00:00, 47.64it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 14 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 42.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model to model.pth\n"]}],"source":["output_link = 'model.pth'\n","ouput_fine_tune_link = 'model-fine-tune.pth'\n","main(mode=\"transfer\", output_file=output_link, device= device)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-12-26T16:41:07.107627Z","iopub.status.busy":"2023-12-26T16:41:07.106729Z","iopub.status.idle":"2023-12-26T17:14:37.362332Z","shell.execute_reply":"2023-12-26T17:14:37.361351Z","shell.execute_reply.started":"2023-12-26T16:41:07.107595Z"},"id":"qSHw5gxky5GN","outputId":"13456d8b-b5a7-4ba7-854a-09af64431433","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["load weight from /kaggle/working/model.pth\n","BERTClass2(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=320, bias=True)\n","  (relu1): ReLU()\n","  (dropout1): Dropout(p=0.3, inplace=False)\n","  (fc2): Linear(in_features=320, out_features=160, bias=True)\n","  (relu2): ReLU()\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (fc3): Linear(in_features=160, out_features=18, bias=True)\n",")\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 0 train/loss = 0.018: 100%|██████████| 549/549 [00:43<00:00, 12.60it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 0 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 39.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.00486223662884927\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 1 train/loss = 0.017: 100%|██████████| 549/549 [00:43<00:00, 12.72it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 1 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 39.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.017684887459807074\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 2 train/loss = 0.016: 100%|██████████| 549/549 [00:43<00:00, 12.77it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 2 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 39.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.025498007968127484\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 3 train/loss = 0.015: 100%|██████████| 549/549 [00:43<00:00, 12.69it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 3 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 39.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.016025641025641024\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 4 train/loss = 0.014: 100%|██████████| 549/549 [00:42<00:00, 12.77it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 4 val/loss = 0.017: 100%|██████████| 49/49 [00:01<00:00, 39.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.030015797788309633\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 5 train/loss = 0.013: 100%|██████████| 549/549 [00:43<00:00, 12.70it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 5 val/loss = 0.019: 100%|██████████| 49/49 [00:01<00:00, 38.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.06363636363636364\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 6 train/loss = 0.013: 100%|██████████| 549/549 [00:43<00:00, 12.74it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 6 val/loss = 0.018: 100%|██████████| 49/49 [00:01<00:00, 39.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.052388289676425275\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 7 train/loss = 0.012: 100%|██████████| 549/549 [00:43<00:00, 12.74it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 7 val/loss = 0.019: 100%|██████████| 49/49 [00:01<00:00, 33.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.057014253563390856\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 8 train/loss = 0.011: 100%|██████████| 549/549 [00:43<00:00, 12.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 8 val/loss = 0.020: 100%|██████████| 49/49 [00:01<00:00, 37.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.07871720116618076\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 9 train/loss = 0.011: 100%|██████████| 549/549 [00:43<00:00, 12.74it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 9 val/loss = 0.019: 100%|██████████| 49/49 [00:01<00:00, 38.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.07888970051132213\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 10 train/loss = 0.010: 100%|██████████| 549/549 [00:43<00:00, 12.74it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 10 val/loss = 0.019: 100%|██████████| 49/49 [00:01<00:00, 38.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.0841799709724238\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 11 train/loss = 0.010: 100%|██████████| 549/549 [00:43<00:00, 12.69it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 11 val/loss = 0.020: 100%|██████████| 49/49 [00:01<00:00, 38.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.1079187105816398\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 12 train/loss = 0.009: 100%|██████████| 549/549 [00:43<00:00, 12.68it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 12 val/loss = 0.021: 100%|██████████| 49/49 [00:01<00:00, 38.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.09388335704125178\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 13 train/loss = 0.009: 100%|██████████| 549/549 [00:43<00:00, 12.72it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 13 val/loss = 0.020: 100%|██████████| 49/49 [00:01<00:00, 38.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.10993657505285412\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 14 train/loss = 0.009: 100%|██████████| 549/549 [00:43<00:00, 12.62it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 14 val/loss = 0.020: 100%|██████████| 49/49 [00:01<00:00, 35.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.1447721179624665\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 15 train/loss = 0.009: 100%|██████████| 549/549 [00:43<00:00, 12.68it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 15 val/loss = 0.022: 100%|██████████| 49/49 [00:01<00:00, 38.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.1304640215198386\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 16 train/loss = 0.008: 100%|██████████| 549/549 [00:43<00:00, 12.66it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 16 val/loss = 0.021: 100%|██████████| 49/49 [00:01<00:00, 38.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.12990527740189445\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 17 train/loss = 0.008: 100%|██████████| 549/549 [00:43<00:00, 12.70it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 17 val/loss = 0.020: 100%|██████████| 49/49 [00:01<00:00, 37.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.14859437751004015\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 18 train/loss = 0.008: 100%|██████████| 549/549 [00:43<00:00, 12.72it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 18 val/loss = 0.022: 100%|██████████| 49/49 [00:01<00:00, 40.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.15661103979460847\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 19 train/loss = 0.008: 100%|██████████| 549/549 [00:43<00:00, 12.73it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 19 val/loss = 0.023: 100%|██████████| 49/49 [00:01<00:00, 38.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.1563100576553491\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 20 train/loss = 0.007: 100%|██████████| 549/549 [00:43<00:00, 12.73it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 20 val/loss = 0.023: 100%|██████████| 49/49 [00:01<00:00, 39.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.1702127659574468\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 21 train/loss = 0.007: 100%|██████████| 549/549 [00:43<00:00, 12.67it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 21 val/loss = 0.023: 100%|██████████| 49/49 [00:01<00:00, 37.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.158890290037831\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 22 train/loss = 0.007: 100%|██████████| 549/549 [00:43<00:00, 12.55it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 22 val/loss = 0.024: 100%|██████████| 49/49 [00:01<00:00, 39.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.16293532338308458\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 23 train/loss = 0.007: 100%|██████████| 549/549 [00:43<00:00, 12.71it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 23 val/loss = 0.023: 100%|██████████| 49/49 [00:01<00:00, 37.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2033898305084746\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 24 train/loss = 0.006: 100%|██████████| 549/549 [00:44<00:00, 12.46it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 24 val/loss = 0.024: 100%|██████████| 49/49 [00:01<00:00, 38.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2098914354644149\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 25 train/loss = 0.006: 100%|██████████| 549/549 [00:43<00:00, 12.70it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 25 val/loss = 0.023: 100%|██████████| 49/49 [00:01<00:00, 37.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.23391812865497075\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 26 train/loss = 0.006: 100%|██████████| 549/549 [00:43<00:00, 12.63it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 26 val/loss = 0.024: 100%|██████████| 49/49 [00:01<00:00, 39.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2346698113207547\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 27 train/loss = 0.006: 100%|██████████| 549/549 [00:43<00:00, 12.54it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 27 val/loss = 0.024: 100%|██████████| 49/49 [00:01<00:00, 37.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.24452133794694345\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 28 train/loss = 0.006: 100%|██████████| 549/549 [00:43<00:00, 12.69it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 28 val/loss = 0.026: 100%|██████████| 49/49 [00:01<00:00, 35.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2718767198679141\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 29 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.58it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 29 val/loss = 0.026: 100%|██████████| 49/49 [00:01<00:00, 37.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2501389660922735\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 30 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 30 val/loss = 0.026: 100%|██████████| 49/49 [00:01<00:00, 38.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2501395868230039\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 31 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.70it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 31 val/loss = 0.026: 100%|██████████| 49/49 [00:01<00:00, 37.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2571588994946659\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 32 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.72it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 32 val/loss = 0.027: 100%|██████████| 49/49 [00:01<00:00, 39.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.25663237682728746\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 33 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.74it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 33 val/loss = 0.029: 100%|██████████| 49/49 [00:01<00:00, 39.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2713301536830949\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 34 train/loss = 0.005: 100%|██████████| 549/549 [00:43<00:00, 12.68it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 34 val/loss = 0.029: 100%|██████████| 49/49 [00:01<00:00, 39.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2814971006852926\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 35 train/loss = 0.005: 100%|██████████| 549/549 [00:42<00:00, 12.81it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 35 val/loss = 0.028: 100%|██████████| 49/49 [00:01<00:00, 38.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2816605359957961\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 36 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.69it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 36 val/loss = 0.029: 100%|██████████| 49/49 [00:01<00:00, 38.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2864610559330894\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 37 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 37 val/loss = 0.028: 100%|██████████| 49/49 [00:01<00:00, 39.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.30973000509424353\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 38 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.76it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 38 val/loss = 0.029: 100%|██████████| 49/49 [00:01<00:00, 37.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2952141057934509\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 39 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 39 val/loss = 0.031: 100%|██████████| 49/49 [00:01<00:00, 39.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2909815645241654\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 40 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.68it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 40 val/loss = 0.032: 100%|██████████| 49/49 [00:01<00:00, 38.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.295017266896892\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 41 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.58it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 41 val/loss = 0.031: 100%|██████████| 49/49 [00:01<00:00, 39.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.28615227388860504\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 42 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.65it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 42 val/loss = 0.034: 100%|██████████| 49/49 [00:01<00:00, 37.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.2748244734202608\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 43 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.69it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 43 val/loss = 0.032: 100%|██████████| 49/49 [00:01<00:00, 38.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.283433133732535\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/549 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 44 train/loss = 0.004: 100%|██████████| 549/549 [00:43<00:00, 12.75it/s]\n","  0%|          | 0/49 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","epoch = 44 val/loss = 0.031: 100%|██████████| 49/49 [00:01<00:00, 37.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation F1 score: 0.29545454545454547\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model to model-fine-tune.pth\n"]}],"source":["main(mode=\"fine-tune\", weight_file= '/kaggle/working/model.pth', output_file=\"model-fine-tune.pth\", device= device)"]},{"cell_type":"markdown","metadata":{"id":"GAXjPX4TVWBu"},"source":["<a id='section06'></a>\n","### Validating the Model\n","\n","As defined above to get a measure of our models performance we are using the following metrics.\n","- Accuracy Score\n","- F1 Micro\n","- F1 Macro"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T17:15:48.604447Z","iopub.status.busy":"2023-12-26T17:15:48.604081Z","iopub.status.idle":"2023-12-26T17:15:49.502089Z","shell.execute_reply":"2023-12-26T17:15:49.501120Z","shell.execute_reply.started":"2023-12-26T17:15:48.604419Z"},"id":"XYquG5KNAU36","outputId":"b6f71177-686a-4e4b-a7f4-6820af548bfa","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["load weight from model-fine-tune.pth\n"]},{"data":{"text/plain":["BERTClass2(\n","  (l1): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc1): Linear(in_features=768, out_features=320, bias=True)\n","  (relu1): ReLU()\n","  (dropout1): Dropout(p=0.3, inplace=False)\n","  (fc2): Linear(in_features=320, out_features=160, bias=True)\n","  (relu2): ReLU()\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (fc3): Linear(in_features=160, out_features=18, bias=True)\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["model = get_model_with_name(name= 'model2', weight_file='model-fine-tune.pth')\n","model.to(device)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T17:15:52.588398Z","iopub.status.busy":"2023-12-26T17:15:52.587788Z","iopub.status.idle":"2023-12-26T17:15:52.599058Z","shell.execute_reply":"2023-12-26T17:15:52.598162Z","shell.execute_reply.started":"2023-12-26T17:15:52.588363Z"},"id":"F6y0uFJKTcqk","trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def apk(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the average precision at k.\n","    This function computes the average prescision at k between two lists of\n","    items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of elements that are to be predicted (order doesn't matter)\n","    predicted : list\n","                A list of predicted elements (order does matter)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The average precision at k over the input lists\n","    \"\"\"\n","    if not actual:\n","        return 0.0\n","\n","    if len(predicted)>k:\n","        predicted = predicted[:k]\n","\n","    score = 0.0\n","    num_hits = 0.0\n","\n","    for i,p in enumerate(predicted):\n","        # first condition checks whether it is valid prediction\n","        # second condition checks if prediction is not repeated\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","\n","    return score / min(len(actual), k)\n","\n","def mapk(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the mean average precision at k.\n","    This function computes the mean average prescision at k between two lists\n","    of lists of items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of lists of elements that are to be predicted\n","             (order doesn't matter in the lists)\n","    predicted : list\n","                A list of lists of predicted elements\n","                (order matters in the lists)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The mean average precision at k over the input lists\n","    \"\"\"\n","    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n","\n","def onehot_mapk(targets, outputs, k):\n","    '''\n","    input:\n","    targets: true label ([0, 1, 0, 1, 1,...])\n","    outputs: predicted label ([0.12, 0.3, 0.51, 0.7, ...])\n","    k: number of top predictions that will take\n","    \n","    return: mapk score\n","    '''\n","    \n","    sorted_prediction_ids = np.argsort(-np.array(outputs),axis=1) #descending sort\n","    top_k_prediction_ids = sorted_prediction_ids[:,:k]\n","    \n","    # get all true label\n","    true_label_list = [ np.where(np.isclose(row, 1.0))[0].tolist() for row in targets]\n","    \n","    return mapk(true_label_list, top_k_prediction_ids)\n"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T17:16:00.803920Z","iopub.status.busy":"2023-12-26T17:16:00.803024Z","iopub.status.idle":"2023-12-26T17:16:00.815579Z","shell.execute_reply":"2023-12-26T17:16:00.814519Z","shell.execute_reply.started":"2023-12-26T17:16:00.803874Z"},"trusted":true},"outputs":[],"source":["def validation():\n","    model.eval()\n","    fin_targets=[]\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","\n","    return fin_outputs, fin_targets"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-12-26T17:16:01.576421Z","iopub.status.busy":"2023-12-26T17:16:01.576054Z","iopub.status.idle":"2023-12-26T17:16:02.695487Z","shell.execute_reply":"2023-12-26T17:16:02.694404Z","shell.execute_reply.started":"2023-12-26T17:16:01.576391Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["outputs, targets = validation()"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-12-26T17:16:04.412433Z","iopub.status.busy":"2023-12-26T17:16:04.412047Z","iopub.status.idle":"2023-12-26T17:16:04.504476Z","shell.execute_reply":"2023-12-26T17:16:04.503688Z","shell.execute_reply.started":"2023-12-26T17:16:04.412400Z"},"id":"Ov1_3R_pAcMo","outputId":"0eaf5f8b-df96-4089-e509-6cdb63f8f02a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy Score = 0.14543114543114544\n","Map@K Score = 0.42746496496496494\n","F1 Score (Micro) = 0.3187358916478555\n","F1 Score (Macro) = 0.1959560683203807\n"]}],"source":["mapk = onehot_mapk(targets, outputs, k = 5)\n","outputs = np.array(outputs) >= 0.5\n","accuracy = metrics.accuracy_score(targets, outputs)\n","f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n","f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n","print(f\"Accuracy Score = {accuracy}\")\n","print(f\"Map@K Score = {mapk}\")\n","print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","print(f\"F1 Score (Macro) = {f1_score_macro}\")"]},{"cell_type":"markdown","metadata":{"id":"lz9O5-AcVWBu"},"source":["<a id='section07'></a>\n","### Saving the Trained Model Artifacts for inference\n","\n","This is the final step in the process of fine tuning the model.\n","\n","The model and its vocabulary are saved locally. These files are then used in the future to make inference on new inputs of news headlines.\n","\n","Please remember that a trained neural network is only useful when used in actual inference after its training.\n","\n","In the lifecycle of an ML projects this is only half the job done. We will leave the inference of these models for some other day."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4219975,"sourceId":7278452,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"029b65c15821481980dcdd409dc07509":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0643118f70a349f7b710fd3ba2f7b309":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc6683e723d14258aa55e89fd1ed484b","placeholder":"​","style":"IPY_MODEL_d778c5decf30449bb889054a5c27d2b1","value":"config.json: 100%"}},"25747b914e48410089948dbae7a72481":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2684e8b15f5441ae82640a8dd6189f3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f06ba7db7e240a1aae07c6284521f93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8a144e98341455db4de7bdc6e71fba7","IPY_MODEL_72426db91e5648b08099fbb68c44d3cc","IPY_MODEL_877485e2ccb64f1c8e4b5a1745028982"],"layout":"IPY_MODEL_d0390137623e472db4623fd8dc36628e"}},"2f3bef22a0434064a44efb3fdb6854b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42476d7b48dc4603a219433ef72d73f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4630f7981b384ed6ad4827c077730315":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ab0a9a8ff645dd9098161091ec2d37":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b2175016c8b404b83ef3aaee33f4a02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc60d1440a684829baead79cb73f4a00","placeholder":"​","style":"IPY_MODEL_42476d7b48dc4603a219433ef72d73f9","value":"tokenizer_config.json: 100%"}},"4f5c15b8959449daa3b93452231bd061":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0643118f70a349f7b710fd3ba2f7b309","IPY_MODEL_edc4d253b55748c7bd01492f6aadb41d","IPY_MODEL_f683968b196f40099929b2961bd7d2cb"],"layout":"IPY_MODEL_dfc4bf9cf4084176a8eec9a742ae0333"}},"52238ba05c314fd087acdfcb66d86018":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59dfed5429824034a6f7d92fd24be2d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5afd07fe16ff49a3b14741e65b5d9bb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d78c7d3bbdc4f2ab6398aef2b4a4995":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ff2b089b333430dbc03216378418b39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5afd07fe16ff49a3b14741e65b5d9bb3","placeholder":"​","style":"IPY_MODEL_dbd4ac156cbd43398c02bd3b64203f97","value":"model.safetensors: 100%"}},"6303582351254df298ed332f9ad12dde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68c49f40eab74f5a855db11c980ff400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cce1a5b72ee43af8cd5e99dd8ab566f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0c5a91a9bb24c5abb5d55b42f219ef5","placeholder":"​","style":"IPY_MODEL_25747b914e48410089948dbae7a72481","value":" 28.0/28.0 [00:00&lt;00:00, 1.76kB/s]"}},"72426db91e5648b08099fbb68c44d3cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c296a0603a441a6a858f573cd925e51","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f03e9f0580cf40d1859f2c036394877f","value":466062}},"72d323d2f243441db62a40cb87242cb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74dcefc076f74ed58bbc9d68dd66672a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6303582351254df298ed332f9ad12dde","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d78c7d3bbdc4f2ab6398aef2b4a4995","value":28}},"783f8d7814f144e78fecf6a24ce0a216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8353642ab3044b46ac69318c49444674":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2c5917566f44ad6915fc5aad39a385a","placeholder":"​","style":"IPY_MODEL_68c49f40eab74f5a855db11c980ff400","value":" 232k/232k [00:00&lt;00:00, 473kB/s]"}},"8551f174a5cc4aa09577b6bfc215a710":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59dfed5429824034a6f7d92fd24be2d9","placeholder":"​","style":"IPY_MODEL_ef2a59914e154a079bf815228cbc5625","value":"vocab.txt: 100%"}},"877485e2ccb64f1c8e4b5a1745028982":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49ab0a9a8ff645dd9098161091ec2d37","placeholder":"​","style":"IPY_MODEL_d32fc9dd58aa42cca3948ba02a24286b","value":" 466k/466k [00:00&lt;00:00, 634kB/s]"}},"8c296a0603a441a6a858f573cd925e51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ed17004d5164e87ab18e0de32c8a077":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95787fc7201249f2b8f199c510baff49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b2175016c8b404b83ef3aaee33f4a02","IPY_MODEL_74dcefc076f74ed58bbc9d68dd66672a","IPY_MODEL_6cce1a5b72ee43af8cd5e99dd8ab566f"],"layout":"IPY_MODEL_72d323d2f243441db62a40cb87242cb7"}},"963eff7f36ff451caf8b465e4ef960fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_029b65c15821481980dcdd409dc07509","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a7834964e5d4019b73a719756c90537","value":440449768}},"9a7834964e5d4019b73a719756c90537":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fe42b3247af469cb43d89bbf68494a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2c5917566f44ad6915fc5aad39a385a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ff6fa6b00d4c5ba318403dfd1b53bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6c47d22180f405cab3983ae9075ffc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbe66af82a384db19971b1e035ee0f71","placeholder":"​","style":"IPY_MODEL_783f8d7814f144e78fecf6a24ce0a216","value":" 440M/440M [00:01&lt;00:00, 274MB/s]"}},"c58803afc6884d70b28918665b57d5a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ff2b089b333430dbc03216378418b39","IPY_MODEL_963eff7f36ff451caf8b465e4ef960fa","IPY_MODEL_b6c47d22180f405cab3983ae9075ffc0"],"layout":"IPY_MODEL_2f3bef22a0434064a44efb3fdb6854b0"}},"cbe66af82a384db19971b1e035ee0f71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc60d1440a684829baead79cb73f4a00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0390137623e472db4623fd8dc36628e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c5a91a9bb24c5abb5d55b42f219ef5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c9fefffd304b6fbe26f23becd59d71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2684e8b15f5441ae82640a8dd6189f3a","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f01a9d7b959a46f99befa2db4fab40e2","value":231508}},"d32fc9dd58aa42cca3948ba02a24286b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d778c5decf30449bb889054a5c27d2b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db9f57a9d3b849658a6fa3c2bce4b5eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8551f174a5cc4aa09577b6bfc215a710","IPY_MODEL_d0c9fefffd304b6fbe26f23becd59d71","IPY_MODEL_8353642ab3044b46ac69318c49444674"],"layout":"IPY_MODEL_fb935a7d0e0c40b59f1d0d42eafe4aaa"}},"dbd4ac156cbd43398c02bd3b64203f97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc6683e723d14258aa55e89fd1ed484b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc4bf9cf4084176a8eec9a742ae0333":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc4d253b55748c7bd01492f6aadb41d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f009a540f6e943fdb88b66abfb94977e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52238ba05c314fd087acdfcb66d86018","value":570}},"ef2a59914e154a079bf815228cbc5625":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f009a540f6e943fdb88b66abfb94977e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f01a9d7b959a46f99befa2db4fab40e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f03e9f0580cf40d1859f2c036394877f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f683968b196f40099929b2961bd7d2cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ed17004d5164e87ab18e0de32c8a077","placeholder":"​","style":"IPY_MODEL_9fe42b3247af469cb43d89bbf68494a7","value":" 570/570 [00:00&lt;00:00, 40.2kB/s]"}},"f8a144e98341455db4de7bdc6e71fba7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4630f7981b384ed6ad4827c077730315","placeholder":"​","style":"IPY_MODEL_a5ff6fa6b00d4c5ba318403dfd1b53bb","value":"tokenizer.json: 100%"}},"fb935a7d0e0c40b59f1d0d42eafe4aaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
